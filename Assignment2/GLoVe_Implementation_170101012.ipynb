{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GLoVe_Implementation_170101012.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjinIbrlsY05",
        "outputId": "0c2357e8-600a-40a2-b07d-24b5442ccb13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Imports\n",
        "import requests\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from IPython.display import display, Markdown\n",
        "import random, more_itertools\n",
        "! pip install sparse\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from random import shuffle\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting sparse\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/82/d58361f8107e8686196b91319edf2c26490667b8340cc229b668ee7a1582/sparse-0.11.2-py2.py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 6.7MB/s \n",
            "\u001b[?25hCollecting numba>=0.49\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/78/31f620c3469287f4255d9a1054bee713cd3596fda2711c392ce3021b3c98/numba-0.51.2-cp36-cp36m-manylinux2014_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 20.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sparse) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from sparse) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.49->sparse) (50.3.2)\n",
            "Collecting llvmlite<0.35,>=0.34.0.dev0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/b7/8a91b513f165e0affdeb975c1fef307c39d1051ce71e8aec1da9dcb317ad/llvmlite-0.34.0-cp36-cp36m-manylinux2010_x86_64.whl (24.6MB)\n",
            "\u001b[K     |████████████████████████████████| 24.6MB 130kB/s \n",
            "\u001b[?25hInstalling collected packages: llvmlite, numba, sparse\n",
            "  Found existing installation: llvmlite 0.31.0\n",
            "    Uninstalling llvmlite-0.31.0:\n",
            "      Successfully uninstalled llvmlite-0.31.0\n",
            "  Found existing installation: numba 0.48.0\n",
            "    Uninstalling numba-0.48.0:\n",
            "      Successfully uninstalled numba-0.48.0\n",
            "Successfully installed llvmlite-0.34.0 numba-0.51.2 sparse-0.11.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1sQ0Nd5r9vv"
      },
      "source": [
        "#Downloading the Corpus\n",
        "en_url_down = \"https://drive.google.com/uc?export=download&id=1H3cNxmsG8k79Vr3FkSa0hkLcC2AGIxSy\"\n",
        "response = requests.get(en_url_down)\n",
        "en_data = response.text\n",
        "en_data = en_data.replace('\\n', '')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR36e1iktIRX",
        "outputId": "39003caa-f66d-46db-b8ee-f662682f79cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "# tokenising the english corpus using NLTK\n",
        "en_tokens = word_tokenize(en_data)\n",
        "display(Markdown(\"###Number of tokens after performing word tokenisation: {}\".format( str(len(en_tokens)))))\n",
        "display(Markdown(\"###Some examples of tokens are:\"))\n",
        "print(en_tokens[0:10])\n",
        "del en_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "###Number of tokens after performing word tokenisation: 19183786",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "###Some examples of tokens are:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['The', 'word', '``', 'atom', \"''\", 'was', 'coined', 'by', 'ancient', 'Greek']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWgzLExZZd_g"
      },
      "source": [
        "# Building vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kfux_ysw4Dos"
      },
      "source": [
        "# function to build the voab out of training data\n",
        "# will return a dictionary of format {word: (id,frequency)}\n",
        "def build_vocab(tokens):\n",
        "    tokensfrequency = {}\n",
        "    for token in tokens:\n",
        "        if token not in tokensfrequency:\n",
        "            tokensfrequency[token] = 1\n",
        "        else:\n",
        "            tokensfrequency[token] += 1\n",
        "    # At the moment taking only 5000 unique tokens2\n",
        "    sorted_en_tokens = sorted(tokensfrequency.items(), key=lambda x: x[1], reverse=True)[:5000]\n",
        "\n",
        "    vocab = {}\n",
        "    i=0\n",
        "    for token, fre in sorted_en_tokens:\n",
        "        vocab[token] = (i, fre)\n",
        "        i += 1\n",
        "    del tokensfrequency\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(en_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbszSfwVZXum"
      },
      "source": [
        "# Cooccurence Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk7VbuqK6GqS"
      },
      "source": [
        "# function to build the co-occurence matrix given window size\n",
        "# will return a matrix with each element cooccurence_matrix[(i,j)] = weight, where i is the main word and j is the context word\n",
        "def build_cooccurence_matrix(tokens, vocab, window_size):\n",
        "    id2token = {id:token for token, (id, fre) in vocab.items()}\n",
        "    token2id = {token:id for token, (id, fre) in vocab.items()}\n",
        "    \n",
        "    cooccurence_matrix = defaultdict(lambda: 0)\n",
        "    \n",
        "    index = 0\n",
        "    for token in tokens:\n",
        "        \n",
        "        # take this token as center_token i.e. main word\n",
        "        center_token = token\n",
        "        \n",
        "        # find all the context words \n",
        "        left_tokens = tokens[max(0, index-window_size) :index]\n",
        "        right_tokens = tokens[index+1:min(len(tokens), window_size+index+1)]\n",
        "        \n",
        "        # temp_dis = window_size\n",
        "        for left_token in left_tokens:\n",
        "            # given in paper\n",
        "            # distance = 1/float(temp_dis)\n",
        "            # temp_dis -= 1\n",
        "            if token in vocab and left_token in vocab:\n",
        "                cooccurence_matrix[(token2id[center_token], token2id[left_token])] += 1\n",
        "        \n",
        "        # temp_dis = 1\n",
        "        for right_token in right_tokens:\n",
        "            # distance = 1/float(temp_dis)\n",
        "            # temp_dis += 1\n",
        "            if token in vocab and right_token in vocab:\n",
        "                cooccurence_matrix[(token2id[center_token], token2id[right_token])] += 1\n",
        "        \n",
        "        index += 1\n",
        "\n",
        "    return cooccurence_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mRBSZqo0xDe"
      },
      "source": [
        "cooccurence_matrix = build_cooccurence_matrix(en_tokens, vocab, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ObRMZEqZns0"
      },
      "source": [
        "# Training GLoVE embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGqlvN9fJfJi"
      },
      "source": [
        "# main function to train GLoVe embeddings\n",
        "def train(vocab, cooccurence_matrix, vector_size, epochs, alpha, x_max, learning_rate):\n",
        "    total_tokens = len(vocab)\n",
        "    # each token will have two word vectors each with dimension vector_size\n",
        "    # one in which it is the main word and in the other it is the context word\n",
        "    W_main = (np.random.randn(total_tokens, vector_size) - 0.5) / float(vector_size)\n",
        "    W_context = (np.random.randn(total_tokens, vector_size) - 0.5)/float(vector_size)\n",
        "\n",
        "    bias_main = (np.random.randn(total_tokens) - 0.5)/float(vector_size)\n",
        "    bias_context = (np.random.randn(total_tokens) - 0.5)/float(vector_size)\n",
        "    \n",
        "    costs = []\n",
        "    for i in range(epochs):\n",
        "        # call iterate function to optimize the weight matrices\n",
        "        print(\"Iteration: %i\", i)\n",
        "        cost, W_main, W_context, bias_main, bias_context  = run_iter(vocab,cooccurence_matrix,  W_main, W_context, bias_main, bias_context, alpha, x_max, learning_rate, vector_size)\n",
        "        costs.append(cost)\n",
        "        print(\"Cost: %f\", cost)\n",
        "    return costs, W_main, W_context, bias_main, bias_context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZePVmXarmxBt"
      },
      "source": [
        "# In each iteration compute the cost and change the weights according \n",
        "# to the adagrad optimization\n",
        "def run_iter(vocab, cooccurence_matrix, W_main, W_context, bias_main, bias_context, alpha, x_max, learning_rate, vector_size):\n",
        "    total_tokens = len(vocab)\n",
        "\n",
        "    gradient_matrix_W_main = np.ones((total_tokens, vector_size), dtype = np.float64)\n",
        "    gradient_matrix_W_context = np.ones((total_tokens, vector_size), dtype = np.float64)\n",
        "    gradient_matrix_bias_main = np.ones(total_tokens, dtype = np.float64)\n",
        "    gradient_matrix_bias_context = np.ones(total_tokens, dtype = np.float64)\n",
        "\n",
        "    total_cost = 0\n",
        "    for w_main, (i, _) in vocab.items():\n",
        "        for w_context, (j, _) in vocab.items():\n",
        "            if (i, j) in cooccurence_matrix:\n",
        "\n",
        "                cooccurence = cooccurence_matrix[(i, j)]\n",
        "                f_x = (cooccurence/x_max) ** alpha if cooccurence < x_max else 1\n",
        "            \n",
        "                cost = f_x * ((W_main[i]).dot(W_context[j]) + bias_main[i] + bias_context[j] - np.log(cooccurence)) ** 2\n",
        "                \n",
        "                # this is done for the ease of computation\n",
        "                total_cost += 0.5 * cost        \n",
        "\n",
        "                gradient_w_main = f_x * ((W_main[i]).dot(W_context[j]) + bias_main[i] + bias_context[j] - np.log(cooccurence)) * W_context[j]\n",
        "                gradient_w_context = f_x * ((W_main[i]).dot(W_context[j]) + bias_main[i] + bias_context[j] - np.log(cooccurence)) * W_main[i]\n",
        "\n",
        "                gradient_bias_main = f_x * ((W_main[i]).dot(W_context[j]) + bias_main[i] + bias_context[j] - np.log(cooccurence))\n",
        "                gradient_bias_context = f_x * ((W_main[i]).dot(W_context[j]) + bias_main[i] + bias_context[j] - np.log(cooccurence))\n",
        "\n",
        "\n",
        "                W_main[i] = W_main[i] - (learning_rate*gradient_w_main)/np.sqrt(gradient_matrix_W_main[i])\n",
        "                W_context[j] = W_context[j] - (learning_rate*gradient_w_context)/np.sqrt(gradient_matrix_W_context[j])\n",
        "\n",
        "                bias_main[i] = bias_main[i] - (learning_rate*gradient_bias_main)/np.sqrt(gradient_matrix_bias_main[i])\n",
        "                bias_context[j] = bias_context[j] - (learning_rate*gradient_bias_context)/np.sqrt(gradient_matrix_bias_context[j])\n",
        "\n",
        "                gradient_matrix_W_main[i] += np.square(gradient_w_main)\n",
        "                gradient_matrix_W_context[j] += np.square(gradient_w_context)\n",
        "                gradient_matrix_bias_main[i] += gradient_bias_main ** 2\n",
        "                gradient_matrix_bias_context[j] += gradient_bias_context ** 2\n",
        "    \n",
        "    return total_cost, W_main, W_context, bias_main, bias_context\n",
        "\n",
        "\n",
        "def exists(obj, chain):\n",
        "    _key = chain.pop(0)\n",
        "    if _key in obj:\n",
        "        return exists(obj[_key], chain) if chain else obj[_key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TS5EWh6F-9h"
      },
      "source": [
        "WINDOW_SIZE = 5\n",
        "NUM_EPOCHS = 25\n",
        "VECTOR_SIZE = 100\n",
        "alpha = 0.75\n",
        "x_max = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxUEfEuJHYQZ"
      },
      "source": [
        "costs, W_main, W_context, bias_main, bias_context =  train(vocab, cooccurence_matrix, VECTOR_SIZE, NUM_EPOCHS, alpha, x_max, learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kveqny1yZByA"
      },
      "source": [
        "# Spearman’s rank correlation corfficient "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLu2CWF-lM3W",
        "outputId": "8e0e5985-51ec-4373-dc80-20df01c164d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# downloading the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! cp -R '/content/drive/My Drive/web' ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 100)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX1kFasCGe4I",
        "outputId": "60160aee-473e-486b-866b-22b2942c6a27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from web.datasets.similarity import fetch_MEN, fetch_WS353, fetch_SimLex999\n",
        "from web.embeddings import fetch_GloVe\n",
        "from web.evaluate import evaluate_similarity\n",
        "from six import iteritems\n",
        "w_glove = fetch_GloVe(corpus=\"wiki-6B\", dim=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset created in /root/web_data/embeddings\n",
            "\n",
            "Downloading data from http://nlp.stanford.edu/data/glove.6B.zip ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 862M/862M [06:28<00:00, 2.22Mb/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...done. (388 seconds, 6 min)\n",
            "Extracting data from /root/web_data/embeddings/glove.6B/glove.6B.zip...\n",
            "   ...done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTKA7c67GnZN"
      },
      "source": [
        "# This particular section is adapted from \n",
        "# https://github.com/kudkudak/word-embeddings-benchmarks/blob/master/examples/evaluate_similarity.py\n",
        "# Define tasks\n",
        "tasks = {\n",
        "    \"MEN\": fetch_MEN(),\n",
        "    \"WS353\": fetch_WS353()\n",
        "}\n",
        "\n",
        "# storing W_main vectors in vocab_vector\n",
        "vocab_vector = {}\n",
        "for word, (id, _) in vocab.items():\n",
        "    vocab_vector[word] = W_main[id]\n",
        "\n",
        "\n",
        "subset_WS353 = [[],[]]\n",
        "name = \"WS353\"\n",
        "for i in range(len(tasks[name].X)):\n",
        "    # taking only the subset of dataset \n",
        "    # i.e. the tokens frrom W3S53 which are also present in our vocab\n",
        "    if tasks[name].X[i][0] in vocab_vector and tasks[name].X[i][1] in vocab_vector:\n",
        "        subset_WS353[0].append([tasks[name].X[i][0], tasks[name].X[i][1]])\n",
        "        subset_WS353[1].append(tasks[name].y[i])\n",
        "\n",
        "subset_WS353[0] = np.array(subset_WS353[0])\n",
        "subset_WS353[1] = np.array(subset_WS353[1])\n",
        "\n",
        "subset_MEN = [[], []]\n",
        "name = \"MEN\"\n",
        "\n",
        "for i in range(len(tasks[name].X)):\n",
        "    # taking only the subset of dataset \n",
        "    # i.e. the tokens frrom W3S53 which are also present in our vocab\n",
        "    if tasks[name].X[i][0] in vocab_vector and tasks[name].X[i][1] in vocab_vector:\n",
        "        subset_MEN[0].append([tasks[name].X[i][0], tasks[name].X[i][1]])\n",
        "        subset_MEN[1].append(tasks[name].y[i][0])\n",
        "\n",
        "subset_MEN[0] = np.array(subset_MEN[0])\n",
        "subset_MEN[1] = np.array(subset_MEN[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7cq6dEqNCOd",
        "outputId": "9958a31c-c293-4cb2-eb0c-82bbd7a1a221",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (\"Spearman correlation of our model of scores on {} {}\".format(\"WS353\", evaluate_similarity(vocab_vector, subset_WS353[0], subset_WS353[1])))\n",
        "print (\"Spearman correlation of Stanford of scores on {} {}\".format(\"WS353\", evaluate_similarity(w_glove, subset_WS353[0], subset_WS353[1])))\n",
        "print (\"Spearman correlation of our model scores on {} {}\".format(\"MEN\", evaluate_similarity(vocab_vector, subset_MEN[0], subset_MEN[1])))\n",
        "print (\"Spearman correlation of stanfords scores on {} {}\".format(\"MEN\", evaluate_similarity(w_glove, subset_MEN[0], subset_MEN[1])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/web/evaluate.py:336: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  A = np.vstack(w.get(word, mean_vector) for word in X[:, 0])\n",
            "/content/web/evaluate.py:337: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  B = np.vstack(w.get(word, mean_vector) for word in X[:, 1])\n",
            "Missing 10 words. Will replace them with mean vector\n",
            "Spearman correlation of our model of scores on WS353 0.114336386249 \n",
            "Spearman correlation of Stanford of scores on WS353 0.57896566434124226 \n",
            "Spearman correlation of our model scores on MEN 0.1335435436359873 \n",
            "Spearman correlation of stanfords scores on MEN 0.68465287102879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XepcJUwYOOKE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}