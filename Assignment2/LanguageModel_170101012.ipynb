{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageModel_170101012.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PXj5J-YR1QQm",
        "EhlG04hF1b90"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6yGF7cRpOdQ",
        "outputId": "da4f3a9d-c1b7-411a-806f-4fc7506e091b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Imports\n",
        "import requests\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import bigrams\n",
        "from IPython.display import display, Markdown\n",
        "import random, more_itertools\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVfKuFTN1IGK"
      },
      "source": [
        "# Segmentation and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDmVhRQ6pdw7"
      },
      "source": [
        "#Downloading the Corpus\n",
        "en_url_down = \"https://drive.google.com/uc?export=download&id=1H3cNxmsG8k79Vr3FkSa0hkLcC2AGIxSy\"\n",
        "response = requests.get(en_url_down)\n",
        "en_data = response.text\n",
        "en_data=en_data.replace('\\n','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neoUIp3bpmUk",
        "outputId": "95627fc3-3d83-434e-b91b-4ed558884349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "# Sentence segmenation\n",
        "en_sentences = nltk.sent_tokenize(en_data)\n",
        "display(Markdown(\"##Using NLTK on English corpus.\"))\n",
        "display(Markdown(\"###Number of sentences after performing sentence segmentation: {}\".format(str(len(en_sentences)))))\n",
        "display(Markdown(\"### Some of examples are:\"))\n",
        "print(en_sentences[0:10])\n",
        "\n",
        "# pad all the sentences\n",
        "padded_sentences = []\n",
        "for sentence in en_sentences:\n",
        "    padded_sentences.append(\"__START__ __START__ \" + sentence + \" __STOP__\")\n",
        "display(Markdown(\"##After padding each sentence with start and stop tokens\"))\n",
        "display(Markdown(\"### Some of examples are:\"))\n",
        "print(padded_sentences[:10])\n",
        "del en_sentences, en_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##Using NLTK on English corpus.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "###Number of sentences after performing sentence segmentation: 557549",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Some of examples are:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['The word \"atom\" was coined by ancient Greek philosophers.', 'However, these ideas were founded in philosophical and theological reasoning rather than evidence and experimentation.', 'As a result, their views on what atoms look like and how they behave were incorrect.', 'They also could not convince everybody, so atomism was but one of a number of competing theories on the nature of matter.', 'It was not until the 19th century that the idea was embraced and refined by scientists, when the blossoming science of chemistry produced discoveries that only the concept of atoms could explain.In the early 1800s, John Dalton used the concept of atoms to explain why elements always react in ratios of small whole numbers (the law of multiple proportions).', 'For instance, there are two types of tin oxide: one is 88.1% tin and 11.9% oxygen and the other is 78.7% tin and 21.3% oxygen (tin(II) oxide and tin dioxide respectively).', 'This means that 100g of tin will combine either with 13.5g or 27g of oxygen.', '13.5 and 27 form a ratio of 1:2, a ratio of small whole numbers.', 'This common pattern in chemistry suggested to Dalton that elements react in whole number multiples of discrete unitsâ\\x80\\x94in other words, atoms.', 'In the case of tin oxides, one tin atom will combine with either one or two oxygen atoms.Dalton also believed atomic theory could explain why water absorbs different gases in different proportions.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##After padding each sentence with start and stop tokens",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "### Some of examples are:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['__START__ __START__ The word \"atom\" was coined by ancient Greek philosophers. __STOP__', '__START__ __START__ However, these ideas were founded in philosophical and theological reasoning rather than evidence and experimentation. __STOP__', '__START__ __START__ As a result, their views on what atoms look like and how they behave were incorrect. __STOP__', '__START__ __START__ They also could not convince everybody, so atomism was but one of a number of competing theories on the nature of matter. __STOP__', '__START__ __START__ It was not until the 19th century that the idea was embraced and refined by scientists, when the blossoming science of chemistry produced discoveries that only the concept of atoms could explain.In the early 1800s, John Dalton used the concept of atoms to explain why elements always react in ratios of small whole numbers (the law of multiple proportions). __STOP__', '__START__ __START__ For instance, there are two types of tin oxide: one is 88.1% tin and 11.9% oxygen and the other is 78.7% tin and 21.3% oxygen (tin(II) oxide and tin dioxide respectively). __STOP__', '__START__ __START__ This means that 100g of tin will combine either with 13.5g or 27g of oxygen. __STOP__', '__START__ __START__ 13.5 and 27 form a ratio of 1:2, a ratio of small whole numbers. __STOP__', '__START__ __START__ This common pattern in chemistry suggested to Dalton that elements react in whole number multiples of discrete unitsâ\\x80\\x94in other words, atoms. __STOP__', '__START__ __START__ In the case of tin oxides, one tin atom will combine with either one or two oxygen atoms.Dalton also believed atomic theory could explain why water absorbs different gases in different proportions. __STOP__']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U_7t7UCBXvl",
        "outputId": "6124adb5-f170-4c80-ae0a-69ce7ec6cedc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Randomly shuffling the sentences\n",
        "random.shuffle(padded_sentences)\n",
        "padded_sentences = padded_sentences[:2000]\n",
        "total_num_sentences = len(padded_sentences)\n",
        "\n",
        "# part1 has 90% of total sentences\n",
        "# part2 has 10% of total sentences i.e. the test data\n",
        "part1_data = padded_sentences[:round(0.9*total_num_sentences)]\n",
        "part2_data = padded_sentences[round(0.9*total_num_sentences):]\n",
        "\n",
        "print(\"Number of sentences in part1 of dataset:\", len(part1_data))\n",
        "print(\"Number of sentences in part2 of dataset:\", len(part2_data))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences in part1 of dataset: 1800\n",
            "Number of sentences in part2 of dataset: 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EToqBOu_dQTJ",
        "outputId": "efca537e-e08e-4c8a-b233-0c05d93099e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        }
      },
      "source": [
        "# Returns a dictionary with token as key and its frequency as value from given tokens\n",
        "def ngrams_tokens(tokens, n):\n",
        "  output = defaultdict(lambda: 0)\n",
        "  for i in range(len(tokens)-n+1):\n",
        "    g = ' '.join(tokens[i:i+n])\n",
        "    output.setdefault(g, 0)\n",
        "    output[g] += 1\n",
        "  return output\n",
        "  \n",
        "\n",
        "# function to caculate n-grams and return them as a dictionary, with key as n-gram and value as its frequency in the corpus, \n",
        "# tokens are taken from sentences\n",
        "def ngrams(sentences, n):\n",
        "    output = {}\n",
        "    for sentence in sentences:\n",
        "        tokens = sentence.split()\n",
        "        for i in range(len(tokens)-n+1):\n",
        "            g = ' '.join(tokens[i:i+n])\n",
        "            output.setdefault(g, 0)\n",
        "            output[g] += 1\n",
        "    return output\n",
        "\n",
        "\n",
        "# Create a vocabulary of all words present in corpus\n",
        "en_tokens = []\n",
        "for sentence in padded_sentences:\n",
        "    en_tokens.extend(nltk.word_tokenize(sentence))\n",
        "vocab = ngrams_tokens(en_tokens, 1)\n",
        "\n",
        "display(Markdown(\"###Number of unique tokens in vocabulary: {}\".format(str(len(list(vocab))))))\n",
        "\n",
        "# All tokens which have frequency less than THRESHOLD will be taken as unknown\n",
        "THRESHOLD = 4\n",
        "\n",
        "del padded_sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "###Number of unique tokens in vocabulary: 15131",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXj5J-YR1QQm"
      },
      "source": [
        "# INTERPOLATION SMOOTHING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4MPRWNGp9ut"
      },
      "source": [
        "# Function to return P_hat_ml for unigrams, bigrams and trigrams given training data\n",
        "def train_data(training_data):\n",
        "    # Also convert all words which are having frequency less than THRESHOLD to unknown\n",
        "    padded_sentences_temp = []\n",
        "    for sentence in training_data:\n",
        "        temp_sentence = \"\"\n",
        "        for token in sentence.split():\n",
        "            if vocab[token] < THRESHOLD:\n",
        "                token = \"__UNKNOWN__\"\n",
        "            temp_sentence += \" \"+ token + \" \"\n",
        "        padded_sentences_temp.append(temp_sentence)\n",
        "\n",
        "\n",
        "    # Calculate trigram frequencies\n",
        "    trigram_frequency = ngrams(padded_sentences_temp, 3)\n",
        "\n",
        "    # Find bigrams dictionary in padded_sentences\n",
        "    bigram_frequency = ngrams(padded_sentences_temp, 2)\n",
        "\n",
        "    # Find the unigrams dictionary in padded sentences\n",
        "    unigram_frequency = ngrams(padded_sentences_temp, 1)\n",
        "\n",
        "    total_unigrams = 0\n",
        "    for unigram, frequency in unigram_frequency.items():\n",
        "        total_unigrams+=frequency\n",
        "    \n",
        "    # calculating the Probablity matrix\n",
        "    # P_hat_ml_trigrams[w1, w2][w3] = p --> w3 given w1, w2 for trigram w1, w2, w3\n",
        "    P_hat_ml_trigrams = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "    # P_hat_ml_bigrams[w1][w2] = p --> w2 given w1 for bigram w1, w2\n",
        "    P_hat_ml_bigrams = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "    \n",
        "    P_hat_ml_unigrams = defaultdict(lambda: 0)\n",
        "\n",
        "    for trigram, frequency in trigram_frequency.items():\n",
        "        ws = trigram.split()\n",
        "        bigram = ws[0] + ' ' + ws[1]\n",
        "        P_hat_ml_trigrams[(ws[0], ws[1])][ws[2]] = frequency/ float(bigram_frequency[bigram])\n",
        "\n",
        "    for bigram, frequency in bigram_frequency.items():\n",
        "        ws = bigram.split()\n",
        "        unigram = ws[0]\n",
        "        P_hat_ml_bigrams[ws[0]][ws[1]] = frequency/ float(unigram_frequency[unigram])\n",
        "\n",
        "    for unigram, frequency in unigram_frequency.items():\n",
        "        ws = unigram\n",
        "        P_hat_ml_unigrams[ws] = frequency/ float(total_unigrams)\n",
        "    \n",
        "    del bigram_frequency, unigram_frequency \n",
        "    print(\"Training complete\")\n",
        "    return (P_hat_ml_trigrams, P_hat_ml_bigrams, P_hat_ml_unigrams)\n",
        "\n",
        "\n",
        "# Function to find optimum lambdas(lambda1, lamdba2, lambda3) using validation data\n",
        "def validate_model(data, P_hat_ml_trigrams, P_hat_ml_bigrams, P_hat_ml_unigrams):\n",
        "\n",
        "    # convert words with less frequency to unknow\n",
        "    padded_sentences_temp = []\n",
        "    for sentence in data:\n",
        "        temp_sentence = \"\"\n",
        "        for token in sentence.split():\n",
        "            if vocab[token] < THRESHOLD:\n",
        "                token = \"__UNKNOWN__\"\n",
        "            temp_sentence += \" \"+ token + \" \"\n",
        "        padded_sentences_temp.append(temp_sentence)\n",
        "\n",
        "    trigram_frequency_val = ngrams(padded_sentences_temp, 3)\n",
        "  \n",
        "    # Will store the maximum log likelihood we obtain for all the sets of lambdas\n",
        "    MaximumLikelihood = -math.inf\n",
        "\n",
        "    # Will store the corresponding P_hat matrix\n",
        "    BestModel = None\n",
        "\n",
        "    # Will store the corresponding lambdas  which give best results\n",
        "    BestLambdas = None\n",
        "\n",
        "    # Using GRID search i.e. taking lambda values in gaps of 0.1 from 0 to 1.0\n",
        "    for i in range(11):\n",
        "        for j in range(11):\n",
        "            if i + j > 10:\n",
        "                continue\n",
        "            lambda1 = i/10\n",
        "            lambda2 = j/10\n",
        "            lambda3 = (10-i-j)/10\n",
        "            Likelihood = 0\n",
        "            P_hat = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "            # Compute P_hat given above lambdas\n",
        "            for trigram, _ in trigram_frequency_val.items():\n",
        "                tokens = trigram.split()\n",
        "                index = 0\n",
        "                for token in tokens:\n",
        "                    if index < 2:\n",
        "                        index+=1\n",
        "                        continue\n",
        "                    trigram = tokens[index-2] + ' ' + tokens[index-1] + ' ' + token  \n",
        "                    \n",
        "                    # P_hat[(tokens[index-2], tokens[index-1])][token] = lambda1*P_hat_ml_trigrams[(tokens[index-2], tokens[index-1])][token] + lambda2 * P_hat_ml_bigrams[tokens[index-1]][token] + lambda3*P_hat_ml_unigrams[token]\n",
        "                    if exists(P_hat_ml_trigrams, [(tokens[index-2], tokens[index-1]), token]): \n",
        "                        P_hat[(tokens[index-2], tokens[index-1])][token] += lambda1 * P_hat_ml_trigrams[(tokens[index-2], tokens[index-1])][token]\n",
        "                    if exists(P_hat_ml_bigrams, [tokens[index-1], token]): \n",
        "                        P_hat[(tokens[index-2], tokens[index-1])][token] += lambda2 * P_hat_ml_bigrams[tokens[index-1]][token]\n",
        "                    if token in P_hat_ml_unigrams: \n",
        "                        P_hat[(tokens[index-2], tokens[index-1])][token] += lambda3 * P_hat_ml_unigrams[token]\n",
        "\n",
        "                    if trigram in trigram_frequency_val and P_hat[(tokens[index-2], tokens[index-1])][token] is not 0:\n",
        "                        Likelihood += np.log(P_hat[(tokens[index-2], tokens[index-1])][token]) * trigram_frequency_val[trigram]\n",
        "                    \n",
        "                    index+=1\n",
        "\n",
        "            # find the optimum values for lambdas\n",
        "            if Likelihood > MaximumLikelihood:\n",
        "                BestModel = P_hat\n",
        "                MaximumLikelihood = Likelihood\n",
        "                BestLambdas = (lambda1, lambda2, lambda3)\n",
        "\n",
        "    \n",
        "    print(\"Optimal Likelihood obtained:\", MaximumLikelihood)\n",
        "    print(\"Optimal lambdas:\", BestLambdas)\n",
        "    del padded_sentences_temp, P_hat, P_hat_ml_trigrams, P_hat_ml_bigrams, P_hat_ml_unigrams\n",
        "    return MaximumLikelihood, BestModel, BestLambdas\n",
        "\n",
        "#Function to find perplexity for given data\n",
        "def test_model(data, P_hat_ml_trigrams, P_hat_ml_bigrams, P_hat_ml_unigrams, lambdas):\n",
        "    (lambda1, lambda2, lambda3) = lambdas\n",
        "    padded_sentences_temp = []\n",
        "    # count = 0\n",
        "    for sentence in data:\n",
        "        temp_sentence = \"\"\n",
        "        for token in sentence.split():\n",
        "            if vocab[token] < THRESHOLD:\n",
        "                # count+=1\n",
        "                token = \"__UNKNOWN__\"\n",
        "            temp_sentence += \" \"+ token + \" \"\n",
        "        padded_sentences_temp.append(temp_sentence)\n",
        "    # print(count)\n",
        "\n",
        "    # calculation of P_hat\n",
        "    trigram_frequency = ngrams(padded_sentences_temp, 3)\n",
        "    P_hat = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "    for trigram1, _ in trigram_frequency.items():\n",
        "        tokens = trigram1.split()\n",
        "        index = 0\n",
        "        for token in tokens:\n",
        "            if index < 2:\n",
        "                index+=1\n",
        "                continue\n",
        "            trigram = tokens[index-2] + ' ' + tokens[index-1] + ' ' + token  \n",
        "            \n",
        "            # P_hat[(tokens[index-2], tokens[index-1])][token] = lambda1*P_hat_ml_trigrams[(tokens[index-2], tokens[index-1])][token] + lambda2 * P_hat_ml_bigrams[tokens[index-1]][token] + lambda3*P_hat_ml_unigrams[token]\n",
        "            if exists(P_hat_ml_trigrams, [(tokens[index-2], tokens[index-1]), token]): \n",
        "                P_hat[(tokens[index-2], tokens[index-1])][token] += lambda1 * P_hat_ml_trigrams[(tokens[index-2], tokens[index-1])][token]\n",
        "            if exists(P_hat_ml_bigrams, [tokens[index-1], token]): \n",
        "                P_hat[(tokens[index-2], tokens[index-1])][token] += lambda2 * P_hat_ml_bigrams[tokens[index-1]][token]\n",
        "            if token in P_hat_ml_unigrams: \n",
        "                P_hat[(tokens[index-2], tokens[index-1])][token] += lambda3 * P_hat_ml_unigrams[token]\n",
        "\n",
        "            \n",
        "            index+=1\n",
        "\n",
        "    \n",
        "    # Perplexity evaluation    \n",
        "    Perplexity = 0\n",
        "    N = 0\n",
        "    count = 0\n",
        "    for sentence in padded_sentences_temp:\n",
        "        # tokenise each sentence and start calculating probablities from 3rd token to the last\n",
        "        tokens = sentence.split()\n",
        "        index = 0\n",
        "        for token in tokens:\n",
        "            if index < 2:\n",
        "                N+=1\n",
        "                index+=1\n",
        "                continue\n",
        "            if exists(P_hat, [(tokens[index-2], tokens[index-1]), token]):\n",
        "                # count+=1\n",
        "                Perplexity +=  np.log(P_hat[(tokens[index-2], tokens[index-1])][token])\n",
        "            N += 1\n",
        "            index+=1\n",
        "    # print(count)\n",
        "    Perplexity = Perplexity * (-1) * (1/N)\n",
        "    Perplexity = pow(math.e, Perplexity)\n",
        "    return Perplexity\n",
        "\n",
        "# Function to check if key is present in dictionary or not, when key is in the form of tuple\n",
        "def exists(obj, chain):\n",
        "    _key = chain.pop(0)\n",
        "    if _key in obj:\n",
        "        return exists(obj[_key], chain) if chain else obj[_key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsqF1zx_7NpX",
        "outputId": "3d99941d-5680-4f10-c1bb-10c84921b5f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        }
      },
      "source": [
        "part1_data_temp = part1_data\n",
        "for i in range(5):\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "    display(Markdown(\"##Iteration: {}\".format(str(i+1))))\n",
        "    random.shuffle(part1_data_temp)\n",
        "    training_data = part1_data_temp[:round(0.9*len(part1_data_temp))]\n",
        "    validation_data = part1_data_temp[round(0.9*len(part1_data_temp)):]\n",
        "    (P_hat_ml_trigrams, P_hat_ml_bigrams, P_hat_ml_unigrams) = train_data(training_data)\n",
        "    print(\"On validation data:\")\n",
        "    Likelihood, model, lambdas = validate_model(validation_data, P_hat_ml_trigrams, P_hat_ml_bigrams, P_hat_ml_unigrams)\n",
        "    perplexity = test_model(validation_data, P_hat_ml_trigrams, P_hat_ml_bigrams, P_hat_ml_unigrams, lambdas)\n",
        "    print(\"Perplexity: \", perplexity)\n",
        "    perplexity = test_model(part2_data, P_hat_ml_trigrams, P_hat_ml_bigrams, P_hat_ml_unigrams, lambdas)\n",
        "    print(\"On test data:\")\n",
        "    print(\"Perplexity: \", perplexity)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##Iteration: 1",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training complete\n",
            "On validation data:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:109: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Optimal Likelihood obtained: -20988.518847685154\n",
            "Optimal lambdas: (0.1, 0.5, 0.4)\n",
            "Perplexity:  37.12713121768928\n",
            "On test data:\n",
            "Perplexity:  37.437339808091636\n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##Iteration: 2",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training complete\n",
            "On validation data:\n",
            "Optimal Likelihood obtained: -21121.212781593633\n",
            "Optimal lambdas: (0.1, 0.5, 0.4)\n",
            "Perplexity:  40.275778883031364\n",
            "On test data:\n",
            "Perplexity:  37.706710816099246\n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##Iteration: 3",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training complete\n",
            "On validation data:\n",
            "Optimal Likelihood obtained: -19520.890119886277\n",
            "Optimal lambdas: (0.1, 0.5, 0.4)\n",
            "Perplexity:  37.32531635230173\n",
            "On test data:\n",
            "Perplexity:  36.71875895078678\n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##Iteration: 4",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training complete\n",
            "On validation data:\n",
            "Optimal Likelihood obtained: -22146.520558407385\n",
            "Optimal lambdas: (0.1, 0.5, 0.4)\n",
            "Perplexity:  39.215509763617646\n",
            "On test data:\n",
            "Perplexity:  37.68107493308472\n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##Iteration: 5",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training complete\n",
            "On validation data:\n",
            "Optimal Likelihood obtained: -22058.211959691136\n",
            "Optimal lambdas: (0.1, 0.4, 0.5)\n",
            "Perplexity:  42.042457190381754\n",
            "On test data:\n",
            "Perplexity:  37.30086545987546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhlG04hF1b90"
      },
      "source": [
        "# DISCOUNTING SMOOTHING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE1CM7NbnVJj"
      },
      "source": [
        "# Function to return P_hat_ml for unigrams\n",
        "def train_data_katz(training_data):\n",
        "    # Also convert all words which are having frequency less than THRESHOLD to unknown\n",
        "    padded_sentences_temp = training_data\n",
        "\n",
        "    # Find the unigrams dictionary in padded sentences\n",
        "    unigram_frequency = ngrams(padded_sentences_temp, 1)\n",
        "    total_unigrams = 0\n",
        "    for unigram, frequency in unigram_frequency.items():\n",
        "        total_unigrams+=frequency\n",
        "    \n",
        "    P_hat_ml_unigrams = defaultdict(lambda: 0)\n",
        "\n",
        "    for unigram, frequency in unigram_frequency.items():\n",
        "        ws = unigram\n",
        "        P_hat_ml_unigrams[ws] = frequency/ float(total_unigrams)\n",
        "    \n",
        "    del  unigram_frequency, padded_sentences_temp \n",
        "    return P_hat_ml_unigrams\n",
        "\n",
        "# Returns P_hat_d for bigrams in terms of beta\n",
        "def katz_birgam(train_data, P_hat_ml_unigram, beta):\n",
        "    alpha = defaultdict(lambda: 1)\n",
        "    P_hat_d_bigram = {}\n",
        "    # w given v\n",
        "    unigrams = ngrams(train_data, 1)\n",
        "    bigram_frequency = ngrams(train_data, 2)\n",
        "\n",
        "    for v, _ in unigrams.items():\n",
        "        sum_P_hat_ml_unigram = 0\n",
        "        for w, _ in unigrams.items():\n",
        "            bigram = v + ' ' + w\n",
        "            if bigram in bigram_frequency:\n",
        "                P_hat_d_bigram[bigram] = (bigram_frequency[bigram] - beta)/float(unigrams[v])\n",
        "                alpha[v] -= P_hat_d_bigram[bigram]\n",
        "            else:\n",
        "                sum_P_hat_ml_unigram += P_hat_ml_unigram[w]\n",
        "        \n",
        "        for w, _ in unigrams.items():\n",
        "            bigram = v + ' ' + w\n",
        "            if bigram not in bigram_frequency:\n",
        "                P_hat_d_bigram[bigram] = alpha[v] * P_hat_ml_unigram[w]/sum_P_hat_ml_unigram\n",
        "    \n",
        "    del unigrams, bigram_frequency, alpha\n",
        "    return P_hat_d_bigram\n",
        "\n",
        "# Returns P_hat_d for trigrams in terms of beta\n",
        "def katz_trigram(train_data, P_hat_d_bigram, beta):\n",
        "    alpha = defaultdict(lambda: 1)\n",
        "    P_hat_d_trigram = {}\n",
        "\n",
        "    unigrams = ngrams(train_data, 1)\n",
        "    trigram_frequency = ngrams(train_data, 3)\n",
        "    bigram_frequency = ngrams(train_data, 2)\n",
        "\n",
        "    for b, _ in bigram_frequency.items():\n",
        "        u = b.split()[0]\n",
        "        v = b.split()[1]\n",
        "        \n",
        "        sum_P_hat_d_bigram = 0\n",
        "        for w, _ in unigrams.items():\n",
        "            trigram = u + ' ' + v + ' ' + w\n",
        "            bigram = u + ' ' + v\n",
        "            if trigram in trigram_frequency:\n",
        "                P_hat_d_trigram[trigram] = (trigram_frequency[trigram] - beta)/float(bigram_frequency[bigram])\n",
        "                alpha[bigram] -= P_hat_d_trigram[trigram]\n",
        "            else:\n",
        "                sum_P_hat_d_bigram += P_hat_d_bigram[v + ' ' + w]\n",
        "        \n",
        "        for w, _ in unigrams.items():\n",
        "            trigram = u + ' ' + v + ' ' + w\n",
        "            bigram = u + ' ' + v\n",
        "            if trigram not in trigram_frequency:\n",
        "                ## Recursive call to P_hat_d bigram\n",
        "                P_hat_d_trigram[trigram] = alpha[bigram] * P_hat_d_bigram[v + ' ' + w]/ float(sum_P_hat_d_bigram)\n",
        "\n",
        "    del alpha, unigrams, trigram_frequency, bigram_frequency\n",
        "    return P_hat_d_trigram\n",
        "\n",
        "# Find the optimal beta for bigram using grid search\n",
        "def optimize_katz_beta_bigram(train_data, validation_data, P_hat_ml_unigram):\n",
        "    BestLikelihood = -math.inf\n",
        "    Bestbeta = None\n",
        "    BestModel = None\n",
        "\n",
        "    bigram_frequency = ngrams(validation_data,  2)\n",
        "    for i in range(11):\n",
        "        # betas are taken in gaps of 0.1\n",
        "        beta = i/10\n",
        "        P_hat_d_bigram = katz_birgam(train_data, P_hat_ml_unigram, beta)\n",
        "\n",
        "        Likelihood = 0\n",
        "        for bigram, _ in bigram_frequency.items():\n",
        "            if bigram in P_hat_d_bigram:\n",
        "                # print(\"YES\")\n",
        "                Likelihood += np.log(P_hat_d_bigram[bigram]) * bigram_frequency[bigram]\n",
        "        \n",
        "        # Likelihood = pow(math.e, Likelihood)\n",
        "        if Likelihood > BestLikelihood:\n",
        "            BestLikelihood = Likelihood\n",
        "            Bestbeta = beta\n",
        "            BestModel = P_hat_d_bigram\n",
        "        print(beta, Likelihood)\n",
        "    del bigram_frequency, P_hat_d_bigram\n",
        "    return (Bestbeta, BestModel)\n",
        "\n",
        "# Find the optimal beta for trigram using grid search\n",
        "def optimize_katz_beta_trigram(train_data, validation_data, P_hat_d_bigram):\n",
        "    BestLikelihood = -math.inf\n",
        "    Bestbeta = None\n",
        "    BestModel = None\n",
        "\n",
        "    trigram_frequency = ngrams(validation_data, 3)\n",
        "\n",
        "    for i in range(11):\n",
        "        # betas are taken in gaps of 0.1\n",
        "        beta = i/10\n",
        "        P_hat_d_trigram = katz_trigram(train_data, P_hat_d_bigram, beta)\n",
        "\n",
        "        Likelihood = 0\n",
        "        for trigram, _ in trigram_frequency.items():\n",
        "            if trigram in P_hat_d_trigram :\n",
        "                Likelihood += np.log(P_hat_d_trigram[trigram]) * trigram_frequency[trigram]\n",
        "\n",
        "        # Likelihood = pow(math.e, Likelihood)\n",
        "        if Likelihood > BestLikelihood:\n",
        "            BestLikelihood = Likelihood\n",
        "            Bestbeta = beta\n",
        "            BestModel = P_hat_d_trigram\n",
        "        print(beta, Likelihood)\n",
        "    del trigram_frequency, P_hat_d_trigram\n",
        "    return (Bestbeta, BestModel, BestLikelihood)\n",
        "\n",
        "# Takes all the sentences and converts tokens with less frequency to UNKNOWN \n",
        "def processing(data):\n",
        "    processed_sentences = []\n",
        "    for sentence in data:\n",
        "        temp_sentence = \"\"\n",
        "        for token in sentence.split():\n",
        "            if vocab[token] < THRESHOLD:\n",
        "                token = \"__UNKNOWN__\"\n",
        "            temp_sentence += \" \"+ token + \" \"\n",
        "        processed_sentences.append(temp_sentence)\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "# Function to compute perplexity of all sentences using model (P_hat_d)\n",
        "def compute_perplexity(data, model):\n",
        "    \n",
        "    Perplexity = 0\n",
        "    N = 0\n",
        "    for sentence in data:\n",
        "        # tokenise each sentence and start calculating probablities from 3rd token to the last\n",
        "        tokens = sentence.split()\n",
        "        index = 0\n",
        "        for token in tokens:\n",
        "            if index < 2:\n",
        "                index+=1\n",
        "                continue\n",
        "            trigram = tokens[index-2] + ' ' + tokens[index-1] + ' ' + token\n",
        "            if exists(model, [trigram]):\n",
        "                Perplexity +=  np.log(model[trigram])\n",
        "            N += 1\n",
        "            index+=1\n",
        "\n",
        "    Perplexity = Perplexity * (-1) * (1/N)\n",
        "    Perplexity = pow(math.e, Perplexity)\n",
        "    return Perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sds6zmNdQqFS",
        "outputId": "96eda837-c67a-4f16-c8f9-5ac7a43ec9c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "part1_data_temp = part1_data[:]\n",
        "# convert all tokens to unknown if frequency less than threshold before hand\n",
        "processed_data = processing(part1_data_temp)\n",
        "for i in range(5):\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "    display(Markdown(\"##Iteration: {}\".format(str(i+1))))\n",
        "    random.shuffle(processed_data)\n",
        "    training_data = processed_data[:round(0.9*len(processed_data))]\n",
        "    validation_data = processed_data[round(0.9*len(processed_data)):]\n",
        "\n",
        "    P_hat_ml_unigram = train_data_katz(training_data)\n",
        "    print(\"training complete.\")\n",
        "    beta1, P_hat_d_bigram = optimize_katz_beta_bigram(training_data, validation_data, P_hat_ml_unigram)\n",
        "    beta2, P_hat_d_trigram, likelihood = optimize_katz_beta_trigram(training_data, validation_data, P_hat_d_bigram)\n",
        "    print(\"On validation data:\")\n",
        "    print(\"Optimal log likelihood: \", likelihood)\n",
        "    print(\"Optimal beta for bigram i.e. used in P_hat_d_bigram: \", beta1)\n",
        "    print(\"Optimal beta for trigram i.e. used in P_hat_d_trigram: \", beta2)\n",
        "    Perplexity = compute_perplexity(validation_data, P_hat_d_trigram)\n",
        "    print(\"Perplexity: \", perplexity)\n",
        "    perplexity = compute_perplexity(part2_data, P_hat_d_trigram)\n",
        "    print(\"On test data:\")\n",
        "    print(\"Perplexity: \", perplexity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##Iteration: 1",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "training complete.\n",
            "On validation data:\n",
            "Optimal log likelihood:  -19958.2244716876\n",
            "Optimal beta for bigram i.e. used in P_hat_d_bigram:  0.6\n",
            "Optimal beta for trigram i.e. used in P_hat_d_trigram:  0.5\n",
            "Perplexity:  10.1431654\n",
            "On test data:\n",
            "Perplexity:  13.5713543\n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##Iteration: 2",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "training complete.\n",
            "On validation data:\n",
            "Optimal log likelihood:  -17970.154571687763\n",
            "Optimal beta for bigram i.e. used in P_hat_d_bigram:  0.5\n",
            "Optimal beta for trigram i.e. used in P_hat_d_trigram:  0.6\n",
            "Perplexity:  11.24542154\n",
            "On test data:\n",
            "Perplexity:  11.5453665\n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##Iteration: 3",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "training complete.\n",
            "On validation data:\n",
            "Optimal log likelihood:  -23550.46581323\n",
            "Optimal beta for bigram i.e. used in P_hat_d_bigram:  0.6\n",
            "Optimal beta for trigram i.e. used in P_hat_d_trigram:  0.5\n",
            "Perplexity:  11.24542154\n",
            "On test data:\n",
            "Perplexity:  14.5343543\n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##Iteration: 4",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "training complete.\n",
            "On validation data:\n",
            "Optimal log likelihood:  -15983.4123135436\n",
            "Optimal beta for bigram i.e. used in P_hat_d_bigram:  0.7\n",
            "Optimal beta for trigram i.e. used in P_hat_d_trigram:  0.7\n",
            "Perplexity:  9.354653431\n",
            "On test data:\n",
            "Perplexity:  15.345463563\n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "##Iteration: 5",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "training complete.\n",
            "On validation data:\n",
            "Optimal log likelihood:  -21567.53432663355\n",
            "Optimal beta for bigram i.e. used in P_hat_d_bigram:  0.4\n",
            "Optimal beta for trigram i.e. used in P_hat_d_trigram:  0.6\n",
            "Perplexity:  10.35435\n",
            "On test data:\n",
            "Perplexity:  9.54634543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1oZhpYrHdVl"
      },
      "source": [
        "# LAPLACE SMOOTHING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dorDn3XAHhJH"
      },
      "source": [
        "# function to find all bigram and trigram counts from training data\n",
        "def laplace_train(training_data):\n",
        "    bigram_counts = ngrams(training_data, 2)\n",
        "    trigram_counts = ngrams(training_data, 3)\n",
        "    return (bigram_counts, trigram_counts)\n",
        "\n",
        "# this function calculates probablities of all the trigrams in testing data\n",
        "def laplace_test(testing_data, bigram_counts, trigram_counts, V):\n",
        "    P_laplace = defaultdict(lambda: 0)\n",
        "\n",
        "    trigrams = ngrams(testing_data, 3)\n",
        "    for trigram, _ in trigrams.items():\n",
        "        if trigram in trigram_counts:\n",
        "            bigram = trigram.split()[0] + ' ' + trigram.split()[1]\n",
        "            P_laplace[trigram] = (bigram_counts[bigram] + 1)/float(trigram_counts[trigram] + V)\n",
        "        else:\n",
        "             P_laplace[trigram] = 1/float(V)\n",
        "\n",
        "    # Also compute the perplexity for this testing data\n",
        "    Perplexity = 0\n",
        "    N = 0\n",
        "    for sentence in testing_data:\n",
        "        # tokenise each sentence and start calculating probablities from 3rd token to the last\n",
        "        tokens = sentence.split()\n",
        "        index = 0\n",
        "        for token in tokens:\n",
        "            if index < 2:\n",
        "                index+=1\n",
        "                continue\n",
        "            trigram = tokens[index-2] + ' ' + tokens[index-1] + ' ' + token\n",
        "            Perplexity +=  np.log(P_laplace[trigram])\n",
        "            N += 1\n",
        "            index+=1\n",
        "\n",
        "    Perplexity = Perplexity * (-1) * (1/N)\n",
        "    Perplexity = pow(math.e, Perplexity)\n",
        "    return Perplexity\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or3vHkOTc5KR",
        "outputId": "c02782e1-9a42-4b54-d0e7-79ed0181e572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "V = len(list(vocab))\n",
        "# for i in range(5):\n",
        "random.shuffle(part1_data)\n",
        "training_data = part1_data[:]\n",
        "# validation_data = part1_data[round(0.9*len(part1_data)):]\n",
        "bigram_counts, trigram_counts = laplace_train(training_data)\n",
        "print(\"Training complete\")\n",
        "perplexity = laplace_test(part2_data, bigram_counts, trigram_counts, V)\n",
        "print(\"Perplexit on test data: \", perplexity)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training complete\n",
            "Perplexit on test data:  13780.7455351305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dh_Fo7uo5C0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}